{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-hain/workshop_london_nlp_2023/blob/main/notebooks/workshop_sbert_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8FpeYl5fcmF"
      },
      "outputs": [],
      "source": [
        "# Installing sentence transformer libary to work with SBERT\n",
        "!pip install -qU transformers sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# standard stuff\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Stuff we will need later\n",
        "import os\n",
        "import csv\n",
        "import time"
      ],
      "metadata": {
        "id": "Y3ggrsx6rhoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Similarity\n"
      ],
      "metadata": {
        "id": "tRSR9dZwkIVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Semantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines, which only find documents based on lexical matches, semantic search can also find synonyms.\n",
        "\n",
        "In fact, this type of search makes browsing more complete by understanding almost exactly what the user is trying to ask, instead of simply matching keywords to pages. The idea behind semantic search is to embed all entries in your corpus, which can be sentences, paragraphs, or documents, into a vector space.\n",
        "\n",
        "At search time, the query is embedded into the same vector space and the closest embedding from your corpus is found. These entries should have a high semantic overlap with the query."
      ],
      "metadata": {
        "id": "BPelO6uasgBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of search: Symmetric vs. Asymmetric Semantic Search\n",
        "A critical distinction for your setup is symmetric vs. asymmetric semantic search:\n",
        "\n",
        "For **symmetric** semantic search your query and the entries in your corpus are of about the same length and have the same amount of content. An example would be searching for similar questions: Your query could for example be “How to learn Python online?” and you want to find an entry like “How to learn Python on the web?”. For symmetric tasks, you could potentially flip the query and the entries in your corpus.\n",
        "\n",
        "For **asymmetric** semantic search, you usually have a short query (like a question or some keywords) and you want to find a longer paragraph answering the query. An example would be a query like “What is Python” and you wand to find the paragraph “Python is an interpreted, high-level and general-purpose programming language. Python’s design philosophy …”. For asymmetric tasks, flipping the query and the entries in your corpus usually does not make sense."
      ],
      "metadata": {
        "id": "0iycP4QHsgPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy example"
      ],
      "metadata": {
        "id": "5Md8vVgDlhY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple example of a couple of sentences. Imagine our task is to calculate semantic similarity between them:"
      ],
      "metadata": {
        "id": "7qSPcnHUlaml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"purple is the best city in the forest\",\n",
        "             \"there is an art to getting your way and throwing bananas on to the street is not it\",\n",
        "             \"it is not often you find soggy bananas on the street\",\n",
        "             \"green should have smelled more tranquil but somehow it just tasted rotten\",\n",
        "             \"joyce enjoyed eating pancakes with ketchup\",\n",
        "             \"as the asteroid hurtled toward earth becky was upset her dentist appointment had been canceled\",\n",
        "             \"to get your way you must not bombard the road with yellow fruit\" ]"
      ],
      "metadata": {
        "id": "WqAU8xx-m-4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross- vs Bi-Encoder\n",
        "\n",
        "![](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/Bi_vs_Cross-Encoder.png)\n",
        "\n",
        "* **Bi-Encoders** produce for a given sentence a sentence embedding. We pass to a BERT independently the sentences A and B, which result in the sentence embeddings u and v. These sentence embedding can then be compared using cosine similarity:\n",
        "* In contrast, for a **Cross-Encoder**, we pass both sentences simultaneously to the Transformer network. It produces than an output value between 0 and 1 indicating the similarity of the input sentence pair:"
      ],
      "metadata": {
        "id": "p0ApOToyyMY4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lo1QrBCfcl_"
      },
      "source": [
        "## BERT (Cross-Encoder)\n",
        "\n",
        "Lets we'll take a look at how we can use transformer models (like BERT) to create sentence vectors for calculating similarity. Let's start by defining a few example sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhXWj2pPfcmG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not torch.cuda.is_available():\n",
        "  print(\"Warning: No GPU detected. Processing will be slow. Please add a GPU to this notebook\")"
      ],
      "metadata": {
        "id": "SZ2hrKihrNyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ0ULI98fcmG"
      },
      "source": [
        "Initialize our HF transformer model and tokenizer - using a pretrained BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JvNtY2BfcmI"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE2EXEx7fcmM"
      },
      "source": [
        "Tokenize all of our sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv2OJ7bBfcmM"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer(sentences,\n",
        "                   max_length=128,\n",
        "                   truncation=True,\n",
        "                   padding='max_length',\n",
        "                   return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCU7dhfSfcmN"
      },
      "outputs": [],
      "source": [
        "tokens.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX4pjINhfcmO"
      },
      "outputs": [],
      "source": [
        "tokens['input_ids'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5a8NbhEfcmP"
      },
      "source": [
        "Process our tokenized tensors through the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd3pq0L5fcmP"
      },
      "outputs": [],
      "source": [
        "outputs = model(**tokens)\n",
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swmG3wVTfcmQ"
      },
      "source": [
        "Here we can see the final embedding layer, *last_hidden_state*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpcX75fLfcmQ"
      },
      "outputs": [],
      "source": [
        "embeddings = outputs.last_hidden_state\n",
        "embeddings[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3jaIO7ufcmQ"
      },
      "outputs": [],
      "source": [
        "embeddings[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3t4k-DSfcmR"
      },
      "source": [
        "Here we have our vectors of length *768*, but we see that these are not *sentence vectors* because we have a vector representation for each token in our sequence (128 in total). We need to perform a mean pooling operation to create the sentence vector.\n",
        "\n",
        "The first thing we do is multiply each value in our `embeddings` tensor by its respective `attention_mask` value. The `attention_mask` contains **1s** where we have 'real tokens' (eg not padding tokens), and 0s elsewhere - so this operation allows us to ignore non-real tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ooaMBv9fcmS"
      },
      "outputs": [],
      "source": [
        "mask = tokens['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float()\n",
        "mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuyeNvE8fcmS"
      },
      "outputs": [],
      "source": [
        "mask[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aCjpn3sfcmS"
      },
      "source": [
        "Now we have a masking array that has an equal shape to our output `embeddings` - we multiply those together to apply the masking operation on our outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTraDqFnfcmT"
      },
      "outputs": [],
      "source": [
        "masked_embeddings = embeddings * mask\n",
        "masked_embeddings[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWj1nPO6fcmT"
      },
      "source": [
        "Sum the remaining embeddings along axis 1 to get a total value in each of our 768 values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUqQRdDvfcmT"
      },
      "outputs": [],
      "source": [
        "summed = torch.sum(masked_embeddings, 1)\n",
        "summed.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt85yaLDfcmU"
      },
      "source": [
        "Next, we count the number of values that should be given attention in each position of the tensor (+1 for real tokens, +0 for non-real)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbvetiX7fcmU"
      },
      "outputs": [],
      "source": [
        "counted = torch.clamp(mask.sum(1), min=1e-9)\n",
        "counted.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-b3yWs-fcmU"
      },
      "source": [
        "Finally, we get our mean-pooled values as the `summed` embeddings divided by the number of values that should be given attention, `counted`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58x19xMTfcmU"
      },
      "outputs": [],
      "source": [
        "mean_pooled = summed / counted\n",
        "mean_pooled.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxFoKuS_fcmV"
      },
      "source": [
        "Now we have our sentence vectors, we can calculate the cosine similarity between each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpdRLZ8LfcmV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vxOlp6OfcmV"
      },
      "outputs": [],
      "source": [
        "# convert to numpy array from torch tensor\n",
        "mean_pooled = mean_pooled.detach().numpy()\n",
        "\n",
        "# calculate similarities (will store in array)\n",
        "scores = np.zeros((mean_pooled.shape[0], mean_pooled.shape[0]))\n",
        "for i in range(mean_pooled.shape[0]):\n",
        "    scores[i, :] = cosine_similarity(\n",
        "        [mean_pooled[i]],\n",
        "        mean_pooled\n",
        "    )[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAb1HorxfcmV"
      },
      "outputs": [],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W6IKPO4fcmW"
      },
      "source": [
        "We can visualize these scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S26816sefcmW"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(scores, annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAJRXFdRfcmW"
      },
      "source": [
        "## SBERT: sentence-transformers (bi-encoders)\n",
        "\n",
        "The `sentence-transformers` library allows us to compress all of the above into just a few lines of code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch"
      ],
      "metadata": {
        "id": "9cZqoCKptDM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjxtAmxnfcmX"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj4ZYvJefcmX"
      },
      "source": [
        "We encode the sentences (producing our mean-pooled sentence embeddings) like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB4JgbmLfcmX"
      },
      "outputs": [],
      "source": [
        "sentence_embeddings = model.encode(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPIFb_8qfcmX"
      },
      "source": [
        "And calculate the cosine similarity just like before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYiYRl2wfcmY"
      },
      "outputs": [],
      "source": [
        "# calculate similarities (will store in array)\n",
        "scores = np.zeros((sentence_embeddings.shape[0], sentence_embeddings.shape[0]))\n",
        "for i in range(sentence_embeddings.shape[0]):\n",
        "    scores[i, :] = cosine_similarity(\n",
        "        [sentence_embeddings[i]],\n",
        "        sentence_embeddings\n",
        "    )[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(scores, annot=True)"
      ],
      "metadata": {
        "id": "zdf6DNeaj_m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also writ a small function to find the most similar sentences to each others"
      ],
      "metadata": {
        "id": "vMkdj6LpnxJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the pairs with the highest cosine similarity scores\n",
        "pairs = []\n",
        "for i in range(len(scores)-1):\n",
        "    for j in range(i+1, len(scores)):\n",
        "        pairs.append({'index': [i, j], 'score': scores[i][j]})"
      ],
      "metadata": {
        "id": "J6kvAEafnwgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort scores in decreasing order\n",
        "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)"
      ],
      "metadata": {
        "id": "uLW-ywRQn45l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in pairs[0:10]:\n",
        "    i, j = pair['index']\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], pair['score']))"
      ],
      "metadata": {
        "id": "nz4Jteaxn5AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or do some semantic search:"
      ],
      "metadata": {
        "id": "PR51Jf2ao_H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query sentences:\n",
        "queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']"
      ],
      "metadata": {
        "id": "lb61M8GAo9lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "top_k = min(2, len(sentences))\n",
        "for query in queries:\n",
        "    query_embedding = model.encode(query)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.cos_sim(query_embedding, sentence_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop  most similar sentences in corpus:\")\n",
        "\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        print(sentences[idx], \"(Score: {:.4f})\".format(score))"
      ],
      "metadata": {
        "id": "c48C0vdFsX6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications"
      ],
      "metadata": {
        "id": "gpL9nW6k_o82"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6-vychrgG1N"
      },
      "source": [
        "## Semantic Search using SBERT on Quora Questions dataset\n",
        "\n",
        "* We use the Quora Duplicate Questions dataset, which contains about 500k questions (we only use about 100k):\n",
        "https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs\n",
        "* The main task the dataset is used is to identify duplicated questions.\n",
        "* As embeddings model, we use the SBERT model 'quora-distilbert-multilingual',\n",
        "that it aligned for 100 languages. I.e., you can type in a question in various languages and it will return the closest questions in the corpus (questions in the corpus are mainly in English).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('quora-distilbert-multilingual')"
      ],
      "metadata": {
        "id": "NHXpdk-9t3r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters for download\n",
        "url = \"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\n",
        "dataset_path = \"quora_duplicate_questions.tsv\"\n",
        "max_corpus_size = 100000"
      ],
      "metadata": {
        "id": "OtqlLct6t8Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the dataset exists. If not, download and extract\n",
        "# Download dataset if needed\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(\"Download dataset\")\n",
        "    util.http_get(url, dataset_path)"
      ],
      "metadata": {
        "id": "dUG7D7jet-L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all unique sentences from the file\n",
        "corpus_sentences = set()\n",
        "with open(dataset_path, encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
        "    for row in reader:\n",
        "        corpus_sentences.add(row['question1'])\n",
        "        if len(corpus_sentences) >= max_corpus_size:\n",
        "            break\n",
        "\n",
        "        corpus_sentences.add(row['question2'])\n",
        "        if len(corpus_sentences) >= max_corpus_size:\n",
        "            break"
      ],
      "metadata": {
        "id": "gJlmd4F-t-WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed the sentences\n",
        "corpus_sentences = list(corpus_sentences)\n",
        "print(\"Encode the corpus. This might take a while\")\n",
        "corpus_embeddings = model.encode(corpus_sentences, show_progress_bar=True, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "x4zqW94suCaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4sZymulgFHn"
      },
      "source": [
        "###############################\n",
        "print(\"Corpus loaded with {} sentences / embeddings\".format(len(corpus_sentences)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Czew0ngvyS"
      },
      "source": [
        "# Function that searches the corpus and prints the results\n",
        "def search(inp_question):\n",
        "    start_time = time.time()\n",
        "    question_embedding = model.encode(inp_question, convert_to_tensor=True)\n",
        "    hits = util.semantic_search(question_embedding, corpus_embeddings)\n",
        "    end_time = time.time()\n",
        "    hits = hits[0]  #Get the hits for the first query\n",
        "\n",
        "    print(\"Input question:\", inp_question)\n",
        "    print(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\n",
        "    for hit in hits[0:5]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvq3CUm2hGXg"
      },
      "source": [
        "search(\"How do i write a really good data science project?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92tO8xFxhnWe"
      },
      "source": [
        "#German: How can I learn Python online?\n",
        "search(\"Wie kann ich online python lernen?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTKECBnVhwRO"
      },
      "source": [
        "#Chinese: How can I learn Python online?\n",
        "search(\"如何在线学习python\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Danish:What should I do at the weekend\n",
        "search(\"hvad skal jeg laver om weekenden\")"
      ],
      "metadata": {
        "id": "nTxzm9m8DWRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# French: How can I learn data science really fast?\n",
        "search(\"comment puis-je apprendre la science des données très rapidement?\")"
      ],
      "metadata": {
        "id": "-_nENIPG0b_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Patent search\n",
        "\n",
        "* Intelectual property search and retrieval has many corporate applications.\n",
        "* It also has many applications in our research on technology mapping and forecasting.\n",
        "Check our application for [patent classification](https://github.com/AI-Growth-Lab/PatentSBERTa)\n",
        "* ALso, see former W2V application in [this paper](https://doi.org/10.1016/j.techfore.2022.121559)"
      ],
      "metadata": {
        "id": "V907w43gmkYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "vSsXuImQtl-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import datasets"
      ],
      "metadata": {
        "id": "fShHjAjNtftr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our patent dataset sample\n",
        "patent_dataset = datasets.load_dataset(\"AI-Growth-Lab/patents_claims_1.5m_traim_test\", split=\"test[:5000]\")"
      ],
      "metadata": {
        "id": "39hMZQUEtsqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')"
      ],
      "metadata": {
        "id": "6KYl6DPkEKkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patent_dataset = pd.DataFrame(patent_dataset)"
      ],
      "metadata": {
        "id": "gH9g7OE2vAQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patent_dataset.text.head()"
      ],
      "metadata": {
        "id": "PiP8LhxiHP_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.encode(patent_dataset.text, convert_to_tensor=True, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "atwbJH7fwo7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* No all is embedded. Lets try to retrieve it"
      ],
      "metadata": {
        "id": "Wtnk8LR933xU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that searches the corpus and prints the results\n",
        "def semantic_search(inp_question, n = 5):\n",
        "    start_time = time.time()\n",
        "    question_embedding = model.encode(inp_question, convert_to_tensor=True)\n",
        "    hits = util.semantic_search(question_embedding, embeddings)\n",
        "    end_time = time.time()\n",
        "    hits = hits[0]  #Get the hits for the first query\n",
        "\n",
        "    print(\"Input question:\", inp_question)\n",
        "    print(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\n",
        "    for hit in hits[0:n]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], patent_dataset.text[hit['corpus_id']]))"
      ],
      "metadata": {
        "id": "Rtg0yIYV36Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query sentences:\n",
        "queries = ['an apperatus that connects databases.']"
      ],
      "metadata": {
        "id": "FqB6Ulkp36Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search(queries)"
      ],
      "metadata": {
        "id": "8YZiNmifHY-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Literature search (Your turn)"
      ],
      "metadata": {
        "id": "-RQ7Z11vZZcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is your turn! We could also use this workflow for a search in academic literature.\n",
        "\n",
        "That's your task now, do the following:\n",
        "\n",
        "1. Define a not too big corpus of literature (<5k)\n",
        "2. Download the metadata including abstracts on OpenAlex. (mostly C&P from previous notebook)\n",
        "3. Using an appropriate transformer model, create embeddings for all abstracts.\n",
        "4. Create a simple semantic search application.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hv8TINetZfvW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wolWoXa6aFP4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}