# Workshop London Text Anlaysis 2023: Introduction to transformer models

![](https://jalammar.github.io/images/transformer-ber-ulmfit-elmo.png)

# Sildes

* [Link to Slide Deck](https://openai.com/product#made-for-developers](https://docs.google.com/presentation/d/1KUpX_vW1Y4xQL5NJ-GJeWTCtC9SghuCsMpSeNDOynJE/edit?usp=sharing)https://docs.google.com/presentation/d/1KUpX_vW1Y4xQL5NJ-GJeWTCtC9SghuCsMpSeNDOynJE/edit?usp=sharing)

# Notebooks

## [1.: Introduction to transformers]()

## [2.: SBERT and Semantic search](https://colab.research.google.com/github/daniel-hain/workshop_london_nlp_2023/blob/main/notebooks/workshop_sbert_similarity.ipynb)

Sentence Transformers are a recent breakthrough in natural language processing that can generate dense, high-quality embeddings of sentences, enabling accurate semantic similarity comparisons between sentences. What makes them particularly exciting for businesses and social science applications is their ability to enable more intuitive, meaningful language-based search, content deduplication, and clustering. With Sentence Transformers, businesses can enhance the accuracy of their search engines, provide more accurate recommendations, and reduce redundancy in content databases. Social science researchers can use Sentence Transformers to identify commonalities between texts and to cluster documents to identify trends and topics in large corpora.

## [2.: LLMs and agents](https://colab.research.google.com/github/daniel-hain/workshop_london_nlp_2023/blob/main/LMM_vectordb_agents.ipynb)

To follow along [create an OpenAI API key](https://openai.com/product#made-for-developers) and put 1$ on it :)

# Further ressources

## Literature and Ressources

### Transformer models generally
* [Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.](https://proceedings.neurips.cc/paper/5346-sequence-to-sequence-learning-with-neural-)
* [Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.](https://proceedings.neurips.cc/paper/7181-attention-is-all)
* [The illustrated transformer](https://jalammar.github.io/illustrated-transformer/)
* [Simple transformer LM](https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/Simple_Transformer_Language_Model.ipynb#scrollTo=BstYQU6NkkDA)

### Sbert 
* OG SBERT-Paper [Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.](https://arxiv.org/abs/1908.10084)
* [SBERT Docu](https://www.sbert.net)
* [NLP with SBERT](https://www.pinecone.io/learn/nlp/) - an ebook/course on the use of dense vectors (with SBERT for business applications)
* [SBERT-Training Tutorial](https://huggingface.co/blog/how-to-train-sentence-transformers)
* [BERTopic](https://maartengr.github.io/BERTopic/index.html) - a framework for topic modelling with SBERT embeddings



