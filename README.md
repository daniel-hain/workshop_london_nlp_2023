# Workshop London Text Anlaysis 2023: Introduction to transformer models

![](https://jalammar.github.io/images/transformer-ber-ulmfit-elmo.png)

# Sildes

* [Link to Slide Deck](https://openai.com/product#made-for-developers](https://docs.google.com/presentation/d/1KUpX_vW1Y4xQL5NJ-GJeWTCtC9SghuCsMpSeNDOynJE/edit?usp=sharing)https://docs.google.com/presentation/d/1KUpX_vW1Y4xQL5NJ-GJeWTCtC9SghuCsMpSeNDOynJE/edit?usp=sharing)

# Notebooks

## [1.: Introduction to transformers](https://colab.research.google.com/github/daniel-hain/workshop_london_nlp_2023/blob/main/notebooks/transformers_intro.ipynb)

## [2.: SBERT and Semantic search](https://colab.research.google.com/github/daniel-hain/workshop_london_nlp_2023/blob/main/notebooks/workshop_sbert_similarity.ipynb)

Sentence Transformers are a recent breakthrough in natural language processing that can generate dense, high-quality embeddings of sentences, enabling accurate semantic similarity comparisons between sentences. What makes them particularly exciting for businesses and social science applications is their ability to enable more intuitive, meaningful language-based search, content deduplication, and clustering. With Sentence Transformers, businesses can enhance the accuracy of their search engines, provide more accurate recommendations, and reduce redundancy in content databases. Social science researchers can use Sentence Transformers to identify commonalities between texts and to cluster documents to identify trends and topics in large corpora.

## [2.: LLMs and agents](https://colab.research.google.com/github/daniel-hain/workshop_london_nlp_2023/blob/main/LMM_vectordb_agents.ipynb)

To follow along [create an OpenAI API key](https://openai.com/product#made-for-developers) and put 1$ on it :)

# Own work and ressources

## Publications, Reports & PReprints using Deep NLP

* [Hain, D. S., Jurowetzki, R., Squicciarini, M., & Xu, L. (2023). Unveiling the Neurotechnology Landscape: Scientific Advancements Innovations and Major Trends. UNESCO Report](https://unesdoc.unesco.org/ark:/48223/pf0000386137)
* [Hain, D., Jurowetzki, R., Lee, S., & Zhou, Y. (2023). Machine learning and artificial intelligence for science, technology, innovation mapping and forecasting: Review, synthesis, and applications. Scientometrics, 128(3), 1465-1472.](https://doi.org/10.1007/s11192-022-04628-8)
* [Hain, D., Jurowetzki, R., & Squicciarini, M. (2022). Mapping Complex Technologies via Science-Technology Linkages; The Case of Neuroscience--A transformer based keyword extraction approach. arXiv preprint arXiv:2205.10153.](https://arxiv.org/abs/2205.10153)
* [Bekamiri, H., Hain, D. S., & Jurowetzki, R. (2022). A survey on sentence embedding models performance for patent analysis. arXiv preprint arXiv:2206.02690.](https://arxiv.org/abs/2206.02690)
* [Hain, D. S., Jurowetzki, R., Buchmann, T., & Wolf, P. (2022). A text-embedding-based approach to measuring patent-to-patent technological similarity. Technological Forecasting and Social Change, 177, 121559.](https://doi.org/10.1016/j.techfore.2022.121559)
* [Bekamiri, H., Hain, D. S., & Jurowetzki, R. (2021). Patentsberta: A deep nlp based hybrid model for patent distance and classification using augmented sbert. arXiv preprint arXiv:2103.11933.](https://arxiv.org/abs/2103.11933)
* [Hain, D. S., Jurowetzki, R., Konda, P., & Oehler, L. (2020). From catching up to industrial leadership: towards an integrated market-technology perspective. An application of semantic patent-to-patent similarity in the wind and EV sector. Industrial and Corporate Change, 29(5), 1233-1255.](https://doi.org/10.1093/icc/dtaa021)
* [Rakas, M., & Hain, D. S. (2019). The state of innovation system research: What happens beneath the surface?. Research Policy, 48(9), 103787.](https://doi.org/10.1016/j.respol.2019.04.011) - Some NLP but not deep, though

## More from us

* [Me on github](https://github.com/daniel-hain)
* [AI Growth Lab on Github](https://github.com/AI-Growth-Lab)
* [AI Growth Lab on Huggingface](https://huggingface.co/AAUBS)

# Further Literature and Ressources

## Transformer models generally
* [Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.](https://proceedings.neurips.cc/paper/5346-sequence-to-sequence-learning-with-neural-)
* [Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.](https://proceedings.neurips.cc/paper/7181-attention-is-all)
## Sbert 
* OG SBERT-Paper [Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.](https://arxiv.org/abs/1908.10084)
* [SBERT Docu](https://www.sbert.net)
* [NLP with SBERT](https://www.pinecone.io/learn/nlp/) - an ebook/course on the use of dense vectors (with SBERT for business applications)
* [SBERT-Training Tutorial](https://huggingface.co/blog/how-to-train-sentence-transformers)
* [BERTopic](https://maartengr.github.io/BERTopic/index.html) - a framework for topic modelling with SBERT embeddings





